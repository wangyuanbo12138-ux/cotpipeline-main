# step1_generate.py
import os
from pathlib import Path
from pydantic import ValidationError

# å¯¼å…¥é…ç½®å’Œå·¥å…·
from step0_config import GENERATION_PROMPT_TEMPLATE, DEFAULT_RAW_DIR, GENERATION_MODEL_NAME_QWEN, GENERATION_MODEL_NAME_DS
from utils.file_utils import load_questions, write_jsonl
from utils.api_utils import get_qwen_client, get_deepseek_client
from utils.pydantic_schema import CoT_Answer_Schema

# --- æ ¸å¿ƒå‡½æ•°ï¼šç»“æ„åŒ–è°ƒç”¨ LLM ---
def structured_call(client, model_name: str, question: str) -> dict:
    """
    ä½¿ç”¨ instructor è°ƒç”¨æ¨¡å‹ï¼Œå¼ºåˆ¶è¿”å› CoT_Answer_Schemaã€‚
    """
    if client is None:
        return {"CoT": None, "Answer": None, "error": "Client not initialized or API_KEY missing."}
        
    # æ„å»º Prompt (ä½¿ç”¨ Step 0 ä¸­é€‚é…ç»“æ„åŒ–è¾“å‡ºçš„æ¨¡æ¿)
    prompt = GENERATION_PROMPT_TEMPLATE.format(question=question)
    
    try:
        # æ ¸å¿ƒè°ƒç”¨ï¼šresponse_model å¼ºåˆ¶ç»“æ„åŒ–è¾“å‡º
        # instructor ä¼šåœ¨åå°è‡ªåŠ¨å¤„ç† JSON è§£æå’ŒéªŒè¯
        structured_response: CoT_Answer_Schema = client.chat.completions.create(
            model=model_name,
            response_model=CoT_Answer_Schema, 
            messages=[
                {"role": "system", "content": "You are a warm and empathetic child companion."}, # ç®€å•çš„äººè®¾ç³»ç»Ÿæç¤ºè¯
                {"role": "user", "content": prompt},
            ],
            # max_retries=1: å¦‚æœæ ¼å¼ä¸å¯¹ï¼Œinstructor ä¼šè‡ªåŠ¨è®©æ¨¡å‹é‡è¯•ä¸€æ¬¡
            max_retries=1 
        )
        
        # è¿”å›ä¸€ä¸ªçº¯å­—å…¸ï¼Œæ–¹ä¾¿ JSON åºåˆ—åŒ–
        return {
            "CoT": structured_response.CoT,
            "Answer": structured_response.Answer,
            "error": None
        }
        
    except ValidationError as e:
        # Pydantic éªŒè¯å¤±è´¥ï¼ˆæ ¼å¼ä¾æ—§ä¸å¯¹ï¼‰
        return {"CoT": None, "Answer": None, "error": f"Pydantic Validation Error: {str(e)}"}
    except Exception as e:
        # å…¶ä»– API æˆ–ç½‘ç»œé”™è¯¯
        return {"CoT": None, "Answer": None, "error": f"API Call Error: {type(e).__name__}: {str(e)}"}

def generate_sample(question):
    """ä¸ºå•ä¸ªé—®é¢˜ç”Ÿæˆ Qwen å’Œ Deepseek çš„ç»“æ„åŒ– CoT/Answerã€‚"""
    
    # 1. åˆå§‹åŒ–å®¢æˆ·ç«¯
    qwen_client = get_qwen_client()
    deepseek_client = get_deepseek_client()
    
    # 2. ç»“æ„åŒ–è°ƒç”¨ Qwen æ¨¡å‹
    # ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹å (å¦‚ qwen-max-latest)
    qwen_output = structured_call(qwen_client, GENERATION_MODEL_NAME_QWEN, question)
    
    # 3. ç»“æ„åŒ–è°ƒç”¨ Deepseek æ¨¡å‹
    # æ³¨æ„ï¼šè¿™é‡Œç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²ï¼Œä¿è¯ä¸ä¾èµ– step0_config æ˜¯å¦æ›´æ–°äº† DeepSeek å˜é‡
    deepseek_output = structured_call(deepseek_client, GENERATION_MODEL_NAME_DS, question)
    
    # 4. å°è£…ç»“æœ
    return {
        "question": question,
        "qwen_result": qwen_output,
        "deepseek_result": deepseek_output
    }

def main():
    # 1. ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    Path(DEFAULT_RAW_DIR).mkdir(parents=True, exist_ok=True)
    
    # 2. åŠ è½½é—®é¢˜
    questions_file = "inputs/questions.txt"
    questions = load_questions(questions_file)
    
    if not questions:
        print(f"âŒ é”™è¯¯ï¼šæœªåŠ è½½ä»»ä½•é—®é¢˜ã€‚è¯·æ£€æŸ¥ {questions_file} æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”åŒ…å«å†…å®¹ã€‚")
        return
        
    print(f"--- ğŸš€ Step 1: æ•°æ®ç”Ÿæˆ (å…± {len(questions)} ä¸ªé—®é¢˜) ---")
    print(f"Qwen æ¨¡å‹: {GENERATION_MODEL_NAME_QWEN}")
    print(f"DeepSeek æ¨¡å‹: {GENERATION_MODEL_NAME_DS}")

    # 3. å¾ªç¯å¤„ç†
    samples = []
    for i, q in enumerate(questions):
        print(f"[{i+1}/{len(questions)}] Processing: {q[:30]}...")
        samples.append(generate_sample(q))

    # 4. ä¿å­˜ç»“æœ
    output_path = os.path.join(DEFAULT_RAW_DIR, "raw_data.jsonl")
    write_jsonl(samples, output_path)
    
    print("---------------------------------------------------------")
    print("âœ… Step 1 complete.")
    print(f"Output saved to: {output_path}")

if __name__ == "__main__":
    main()