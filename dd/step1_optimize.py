import os
import json
import time
from pathlib import Path
from pydantic import ValidationError

# å¯¼å…¥é…ç½®
from step0_config import (
    GENERATION_PROMPT_TEMPLATE,
    REFINEMENT_PROMPT_TEMPLATE,
    JUDGE_PROMPT_TEMPLATE,
    DEFAULT_RAW_DIR,
    GENERATION_MODEL_NAME_QWEN, # Qwen æ¨¡å‹å
    GENERATION_MODEL_NAME_DS,   # DeepSeek æ¨¡å‹å
    JUDGE_MODEL_NAME
)
from utils.file_utils import load_questions, write_jsonl
from utils.api_utils import get_qwen_client, get_deepseek_client, get_judge_client
from utils.pydantic_schema import CoT_Answer_Schema, JudgeSchema

# --- æ ¸å¿ƒæµç¨‹å°è£…ï¼šå¯¹å•ä¸ªæ¨¡å‹è¿›è¡Œ"ç”Ÿæˆ-è¯„ä»·-ä¼˜åŒ–" ---
def process_single_model_optimization(model_name, client, model_id, question, judge_client):
    """
    è¾“å…¥ï¼šæ¨¡å‹åç§°ã€å®¢æˆ·ç«¯ã€æ¨¡å‹IDã€é—®é¢˜ã€è£åˆ¤å®¢æˆ·ç«¯
    è¾“å‡ºï¼šåŒ…å« V1, è¯„ä»·, V2 çš„å­—å…¸
    """
    print(f"  ğŸ¤– [{model_name}] æ­£åœ¨è¿›è¡Œä¼˜åŒ–æµç¨‹...")
    
    # --- Round 1: åˆå§‹ç”Ÿæˆ (V1) ---
    # print(f"    1ï¸âƒ£  ç”Ÿæˆåˆå§‹ç‰ˆæœ¬...")
    prompt_v1 = GENERATION_PROMPT_TEMPLATE.format(question=question)
    try:
        v1_resp = client.chat.completions.create(
            model=model_id,
            response_model=CoT_Answer_Schema,
            messages=[
                {"role": "system", "content": "You are AiMe, a warm child companion."},
                {"role": "user", "content": prompt_v1}
            ],
            temperature=0.7, # æ­£å¸¸æ¸©åº¦
            max_retries=3
        )
    except Exception as e:
        print(f"    âŒ {model_name} V1 ç”Ÿæˆå¤±è´¥: {e}")
        return None

    # --- Round 2: è£åˆ¤ç‚¹è¯„ ---
    # print(f"    2ï¸âƒ£  è£åˆ¤æ­£åœ¨æŒ‘åˆº...")
    # æ„é€ ç»™è£åˆ¤çœ‹çš„å†…å®¹
    formatted_output = f"ã€æ€ç»´é“¾ã€‘{v1_resp.CoT}\nã€å›ç­”ã€‘{v1_resp.Answer}"
    judge_prompt = JUDGE_PROMPT_TEMPLATE.format(
        output_a=formatted_output,
        output_b="ï¼ˆæ— å¯¹æ¯”é¡¹ï¼Œè¯·ä½œä¸ºä¸“å®¶å¯¹æ¨¡å‹Aè¿›è¡Œä¸¥æ ¼çš„å•é¡¹è¯„å®¡ï¼ŒæŒ‡å‡ºä¸è¶³ä¹‹å¤„ï¼‰"
    )
    
    try:
        critique_res = judge_client.chat.completions.create(
            model=JUDGE_MODEL_NAME,
            response_model=JudgeSchema,
            messages=[
                {"role": "system", "content": "You are a strict child psychology expert. Be critical."},
                {"role": "user", "content": judge_prompt}
            ],
            temperature=0.3, # è£åˆ¤è¦å†·é™
            max_retries=3
        )
    except Exception as e:
        print(f"    âŒ è£åˆ¤ç‚¹è¯„å¤±è´¥: {e}")
        return None
    
    # æå–ä¿®æ”¹æ„è§
    feedback = f"å…±æƒ…ä¸è¶³ç‚¹ï¼š{critique_res.accuracy_analysis}\nå¼•å¯¼æ”¹è¿›ç‚¹ï¼š{critique_res.reasoning_analysis}"
    
    # --- Round 3: ä¼˜åŒ–ä¿®æ­£ (V2) ---
    # print(f"    3ï¸âƒ£  æ ¹æ®æ„è§é‡å†™...")
    refine_prompt = REFINEMENT_PROMPT_TEMPLATE.format(
        question=question,
        critique=feedback
    )
    
    try:
        v2_resp = client.chat.completions.create(
            model=model_id,
            response_model=CoT_Answer_Schema,
            messages=[
                # è¿™é‡Œçš„ System Prompt ç¨å¾®æ”¹å¾—å¼ºç¡¬ä¸€ç‚¹ï¼Œè¦æ±‚å®ƒæ”¹å˜
                {"role": "system", "content": "You are AiMe. You MUST improve your answer significantly based on the expert feedback."},
                {"role": "user", "content": refine_prompt}
            ],
            temperature=0.8, # â˜… å…³é”®ä¿®æ”¹ï¼šè°ƒé«˜æ¸©åº¦ï¼Œå¢åŠ å˜åŒ–å¹…åº¦
            max_retries=3
        )
    except Exception as e:
        print(f"    âŒ {model_name} V2 ä¿®æ­£å¤±è´¥: {e}")
        v2_resp = None

    return {
        "model_name": model_name,
        "v1_initial": v1_resp.model_dump(),
        "critique": feedback,
        "v2_optimized": v2_resp.model_dump() if v2_resp else "Optimization Failed"
    }

def main():
    Path(DEFAULT_RAW_DIR).mkdir(parents=True, exist_ok=True)
    questions = load_questions("inputs/questions.txt")
    
    # åˆå§‹åŒ–æ‰€æœ‰å®¢æˆ·ç«¯
    qwen_client = get_qwen_client()
    ds_client = get_deepseek_client()
    judge_client = get_judge_client()
    
    optimized_samples = []
    
    print(f"--- ğŸš€ å¼€å§‹åŒæ¨¡å‹è‡ªæˆ‘ä¼˜åŒ– (Dual Self-Correction) ---")
    print(f"ä¼˜åŒ–ç›®æ ‡: Qwen & DeepSeek")
    
    for i, q in enumerate(questions):
        print(f"\n[{i+1}/{len(questions)}] å¤„ç†é—®é¢˜: {q}")
        
        # 1. ä¼˜åŒ– Qwen
        qwen_result = process_single_model_optimization(
            "Qwen", qwen_client, GENERATION_MODEL_NAME_QWEN, q, judge_client
        )
        
        # 2. ä¼˜åŒ– DeepSeek
        ds_result = process_single_model_optimization(
            "DeepSeek", ds_client, GENERATION_MODEL_NAME_DS, q, judge_client
        )
        
        # 3. æ•´åˆç»“æœ
        if qwen_result and ds_result:
            record = {
                "question": q,
                "qwen_data": qwen_result,
                "deepseek_data": ds_result
            }
            optimized_samples.append(record)
            print("  âœ… æœ¬é¢˜åŒæ¨¡å‹ä¼˜åŒ–å®Œæˆï¼")
        
        # é¿å…é€Ÿç‡é™åˆ¶
        time.sleep(1)

    # ä¿å­˜
    output_path = os.path.join(DEFAULT_RAW_DIR, "dual_optimized_data.jsonl")
    write_jsonl(optimized_samples, output_path)
    print(f"\nğŸ’ æœ€ç»ˆä¼˜åŒ–ç»“æœå·²ä¿å­˜è‡³: {output_path}")
    print("æ‚¨å¯ä»¥æŸ¥çœ‹æ–‡ä»¶ï¼Œå¯¹æ¯” v1_initial å’Œ v2_optimized çš„åŒºåˆ«ã€‚")

if __name__ == "__main__":
    main()