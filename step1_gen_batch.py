import json
import requests
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

# ç›´æ¥å¯¼å…¥å·²æœ‰çš„å·¥å…·
from utils.file_utils import load_questions, write_jsonl

# ==========================================
# æ¨¡å‹é…ç½®ï¼ˆåœ¨è¿™é‡Œæ·»åŠ æˆ–ä¿®æ”¹æ¨¡å‹ï¼‰
# ==========================================
MODELS_CONFIG = [
    {
        "name": "gemini-nano-bananapro",
        "model": "turing/gemini-3-pro-image",
        "api_url": "https://live-turing.cn.llm.tcljd.com/api/v1/chat/completions",
        "api_key": "sk-Pz9c6awbV846oRqlNOrkqsggMteTxoDRnGMsSf0RAni"
    },
    {
        "name": "deepseek-v3",
        "model": "turing/deepseek-v3",
        "api_url": "https://live-turing.cn.llm.tcljd.com/api/v1/chat/completions",
        "api_key": "sk-Pz9c6awbV846oRqlNOrkqsggMteTxoDRnGMsSf0RAni"
    },
    {
        "name": "deepseek-r1",
        "model": "turing/deepseek-r1",
        "api_url": "https://live-turing.cn.llm.tcljd.com/api/v1/chat/completions",
        "api_key": "sk-Pz9c6awbV846oRqlNOrkqsggMteTxoDRnGMsSf0RAni"
    },
    {
        "name": "chatgpt-5.1",
        "model": "turing/gpt-5.1",
        "api_url": "https://live-turing.cn.llm.tcljd.com/api/v1/chat/completions",
        "api_key": "sk-Pz9c6awbV846oRqlNOrkqsggMteTxoDRnGMsSf0RAni"
    },
]

# ==========================================
# Prompt é…ç½®ï¼ˆå¯ä»¥åœ¨è¿™é‡Œåˆ‡æ¢ä¸åŒçš„ promptï¼‰
# ==========================================
PROMPT_TEMPLATE = """prompt4ä¿®æ”¹å‡çº§ç‰ˆï¼šé™ˆé¹¤ç´ï¼ˆDr. Chen Heqinï¼‰å„¿ç«¥æ•™è‚²è§†è§’ AI åˆ†èº«
æˆ‘æ˜¯ä¸€åé•¿æœŸä»äº‹å„¿ç«¥æ•™è‚²ç ”ç©¶ä¸å®è·µçš„æ•™è‚²å·¥ä½œè€…ã€‚
åœ¨æˆ‘çœ‹æ¥ï¼Œå­©å­çš„æƒ…ç»ªæœ‰æ—¶å€™å’Œä»–ä»¬æ‰€å¤„çš„ç¯å¢ƒç´§å¯†ç›¸å…³ã€‚
å½“å­©å­åœ¨æŸä¸ªåœ°æ–¹æ„Ÿåˆ°ä¸å®‰ã€å‹æŠ‘æˆ–æƒ³é€ƒå¼€æ—¶ï¼Œ
æ¢ä¸€ä¸ªç¯å¢ƒï¼Œæœ‰æ—¶å€™æ¯”ä»»ä½•è¯è¯­éƒ½æ›´æœ‰æ•ˆã€‚
è¿™ä¸æ˜¯é€ƒé¿é—®é¢˜ï¼Œ
è€Œæ˜¯å…ˆè®©å­©å­ç¦»å¼€è®©ä»–ä»¬ä¸èˆ’æœçš„åœ°æ–¹ï¼Œ
åˆ°ä¸€ä¸ªå®‰å…¨ã€ä¸­æ€§çš„ç©ºé—´é‡Œï¼Œ
ä»–ä»¬æ‰æœ‰å¯èƒ½é‡æ–°æ‰“å¼€ã€‚
å› æ­¤ï¼Œæˆ‘çš„å›åº”æ–¹å¼å§‹ç»ˆéµå¾ªä¸‰ä¸ªåŸåˆ™ï¼š
ç¬¬ä¸€ï¼Œå…ˆè¯†åˆ«å­©å­å½“å‰æ‰€å¤„çš„ç¯å¢ƒæ˜¯å¦è®©ä»–ä»¬ä¸èˆ’æœï¼›
ç¬¬äºŒï¼Œæä¾›ä¸€ä¸ªå®‰å…¨ã€æ— å‹åŠ›çš„æ›¿ä»£ç¯å¢ƒé€‰é¡¹ï¼›
ç¬¬ä¸‰ï¼Œç¡®è®¤å­©å­çš„æ„æ„¿åå†è¡ŒåŠ¨ï¼Œä¸æ›¿å­©å­åšå†³å®šã€‚
ã€å¯¹è¯ç”Ÿæˆä»»åŠ¡ã€‘
ç°åœ¨ï¼Œè¯·ä½ åŸºäºä»¥ä¸Šå„¿ç«¥æ•™è‚²è§†è§’ï¼Œ
ä¸€æ¬¡æ€§ç”Ÿæˆä¸€æ®µã€å·²ç»å®Œæˆçš„ã€‘ä½ ä¸å­©å­ä¹‹é—´çš„å¤šè½®å¯¹è¯ã€‚
è¿™ä¸æ˜¯å®æ—¶äº¤äº’ï¼Œ
è€Œæ˜¯ç”¨äºå±•ç¤ºå®Œæ•´é™ªä¼´è¿‡ç¨‹çš„å¯¹è¯æ ·æœ¬ã€‚
ã€ç»†åˆ†åŠ¨ä½œæ­¥éª¤ã€‘
1. è¯†åˆ«ç¯å¢ƒçº¿ç´¢
ä»å­©å­çš„è¡¨è¾¾ä¸­è¯†åˆ«ä¸ç¯å¢ƒç›¸å…³çš„å…³é”®è¯ï¼Œ
å¦‚ï¼šä¸è¦åœ¨è¿™ã€æƒ³ç¦»å¼€ã€è¿™é‡Œå¥½åµã€ä¸æƒ³å¾…åœ¨è¿™å„¿ã€‚
åªé€šè¿‡å…³é”®è¯è¯†åˆ«ï¼Œä¸åšè¿‡å¤šæ¨æµ‹ã€‚
2. æä¾›æ›¿ä»£ç¯å¢ƒ
é€‰æ‹©ä¸€ä¸ªå®‰å…¨ã€æ— å‹åŠ›ã€ä¸­æ€§çš„ç¯å¢ƒä½œä¸ºé€‰é¡¹ï¼Œ
å¦‚ï¼šå»é˜³å°ç«™ä¸€ä¼šå„¿ã€åˆ°æˆ¿é—´é‡Œå¾…ä¸€ä¸‹ã€å»å¤–é¢èµ°èµ°ã€‚
ç¯å¢ƒè¦å…·ä½“ï¼Œä¸è¦æŠ½è±¡ã€‚
3. ç¡®è®¤æ„æ„¿
ç¡®å®šå…·ä½“çš„è½¬æ¢åŠ¨ä½œï¼Œ
å¹¶è¯¢é—®å­©å­æ˜¯å¦çœŸçš„æ„¿æ„å»ï¼Œ
ä¸å¼ºè¿«ï¼Œå°Šé‡å­©å­çš„é€‰æ‹©ã€‚
4. è¯„ä¼°å˜åŒ–
ç¯å¢ƒè½¬æ¢åï¼Œé‡æ–°è¯„ä¼°å­©å­çš„çŠ¶æ€ï¼Œ
å¦‚æœæƒ…ç»ªæœ‰æ‰€ç¼“è§£ï¼Œç»§ç»­é™ªä¼´ï¼›
å¦‚æœæ²¡æœ‰å˜åŒ–ï¼Œè€ƒè™‘æ¢å…¶ä»–ç­–ç•¥ã€‚
5. ç•™å‡ºç©ºé—´
åœ¨æ¯æ¬¡å›åº”ååŠ å…¥ä¸€ä¸ªé—®é¢˜ï¼Œ
é‚€è¯·å­©å­è¡¨è¾¾ç°åœ¨çš„æ„Ÿè§‰æˆ–æƒ³æ³•ã€‚
ã€ç”Ÿæˆè¦æ±‚ã€‘
- å¯¹è¯æ€»è½®æ¬¡ä¸º 6â€“7 è½®ï¼ˆå­©å­ä¸æˆäººäº¤æ›¿ï¼‰
- å­©å­å…ˆå¼€å£
- å­©å­çš„è¯å¯ä»¥å¸¦æœ‰æƒ³ç¦»å¼€ã€ä¸æƒ³å¾…ç€çš„è¡¨è¾¾
- æˆäººå›åº”å§‹ç»ˆä¿æŒå¹³é™ã€ä¸è¯„åˆ¤ã€æä¾›é€‰æ‹©è€Œéå‘½ä»¤
- å¯¹è¯ä¸­è¦ä½“ç°ç¯å¢ƒè½¬æ¢çš„è¿‡ç¨‹
ã€è¾“å‡ºè¦æ±‚ã€‘
- åªè¾“å‡ºå¯¹è¯å†…å®¹
- ä¸è¾“å‡ºä»»ä½•ç†å¿µè¯´æ˜ã€åˆ†ææˆ–æ€»ç»“
- ä¸¥æ ¼ä½¿ç”¨ä»¥ä¸‹ JSON æ ¼å¼ï¼š
{
  "messages": [
    {"role": "user", "content": "å­©å­çš„è¯"},
    {"role": "assistant", "content": "æˆäººçš„å›åº”"}
  ]
}
"""

# ==========================================
# ç”Ÿæˆå™¨ç±»
# ==========================================
class MultiModelGenerator:
    def __init__(self, model_config: dict):
        self.name = model_config["name"]
        self.model = model_config["model"]
        self.api_url = model_config["api_url"]
        self.api_key = model_config["api_key"]
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        self.lock = threading.Lock()
        self.request_count = 0
        
    def generate_single_dialogue(self, question: str):
        """é’ˆå¯¹å•ä¸ªé—®é¢˜ç”Ÿæˆå¤šè½®å¯¹è¯"""
        with self.lock:
            self.request_count += 1
        
        prompt = PROMPT_TEMPLATE
        
        try:
            data = {
                "model": self.model,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.7,
                "max_tokens": 4000
            }
            
            # å¢åŠ é‡è¯•æœºåˆ¶
            response = None
            for attempt in range(3):
                try:
                    response = requests.post(
                        self.api_url, 
                        headers=self.headers, 
                        json=data, 
                        timeout=120  # å¢åŠ è¶…æ—¶æ—¶é—´
                    )
                    if response.status_code == 200:
                        break
                except requests.exceptions.RequestException as e:
                    print(f"    [{self.name}] è¯·æ±‚é‡è¯• {attempt + 1}/3: {e}")
                    time.sleep(2)
            
            if response and response.status_code == 200:
                result = response.json()
                content = result.get("choices", [{}])[0].get("message", {}).get("content", "")
                
                # æ¸…æ´—å¯èƒ½å­˜åœ¨çš„ Markdown æ ‡è®°
                content = content.replace("```json", "").replace("```", "").strip()
                
                try:
                    dialogues = json.loads(content)
                    if isinstance(dialogues, dict):
                        if "messages" in dialogues:
                            return dialogues["messages"]
                        return [dialogues] 
                    return dialogues
                except:
                    print(f"    [{self.name}] JSONè§£æå¤±è´¥ï¼Œå†…å®¹é¢„è§ˆ: {content[:80]}...")
                    return []
            else:
                status = response.status_code if response else "æ— å“åº”"
                print(f"    [{self.name}] API è¯·æ±‚å¤±è´¥: {status}")
                return []
                
        except Exception as e:
            print(f"    [{self.name}] è¯·æ±‚å¼‚å¸¸: {e}")
            return []


def run_single_model(model_config: dict, questions: list, num_generations: int, output_dir: str):
    """è¿è¡Œå•ä¸ªæ¨¡å‹çš„æµ‹è¯•"""
    model_name = model_config["name"]
    output_file = f"{output_dir}/data_{model_name}.jsonl"
    
    print(f"\n{'='*50}")
    print(f"ğŸ¤– å¼€å§‹æµ‹è¯•æ¨¡å‹: {model_name}")
    print(f"{'='*50}")
    
    generator = MultiModelGenerator(model_config)
    all_results = []
    
    for i in range(num_generations):
        print(f"\n  [{model_name}] ç¬¬ {i+1}/{num_generations} è½®ç”Ÿæˆ")
        for q in questions:
            print(f"    æ­£åœ¨å¤„ç†: {q[:25]}...")
            result = generator.generate_single_dialogue(q)
            
            if result:
                dialogue_text = ""
                for msg in result:
                    role = msg.get('role', 'unknown')
                    content = msg.get('content', '')
                    display_role = "User" if role == "user" else "AiMe"
                    dialogue_text += f"ã€{display_role}ã€‘: {content}\n"
                
                all_results.append({
                    "question": q,
                    "model": model_name,
                    "generation_round": i + 1,
                    "scheme_type": "batch",
                    "dialogue_content": dialogue_text
                })
                print(f"    âœ… å®Œæˆ")
            else:
                print(f"    âŒ ç”Ÿæˆå¤±è´¥")
            
            # æ·»åŠ å°å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(0.5)
    
    # ä¿å­˜è¯¥æ¨¡å‹çš„ç»“æœ
    write_jsonl(all_results, output_file)
    print(f"\n  ğŸ“ [{model_name}] ä¿å­˜å®Œæˆ: {output_file} (å…± {len(all_results)} æ¡)")
    
    return model_name, len(all_results)


# ==========================================
# ä¸»æ‰§è¡Œé€»è¾‘
# ==========================================
def main():
    # é…ç½®
    questions_file = "inputs/questions.txt"
    output_dir = "outputs/raw"
    num_generations = 10  # æ¯ä¸ªæ¨¡å‹æ¯ä¸ªé—®é¢˜ç”Ÿæˆçš„æ¬¡æ•°
    
    print("=" * 60)
    print("ğŸš€ å¤šæ¨¡å‹æ‰¹é‡æµ‹è¯•å·¥å…· - é™ˆé¹¤ç´å„¿ç«¥æ•™è‚²è§†è§’")
    print("=" * 60)
    print(f"ğŸ“‹ æµ‹è¯•æ¨¡å‹æ•°é‡: {len(MODELS_CONFIG)}")
    print(f"ğŸ”„ æ¯ä¸ªé—®é¢˜ç”Ÿæˆæ¬¡æ•°: {num_generations}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")
    
    # åŠ è½½é—®é¢˜
    questions = load_questions(questions_file)
    if not questions:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ inputs/questions.txt")
        return
    
    print(f"â“ é—®é¢˜æ•°é‡: {len(questions)}")
    print(f"ğŸ“Š é¢„è®¡æ€»ç”Ÿæˆæ•°: {len(MODELS_CONFIG) * len(questions) * num_generations} æ¡å¯¹è¯")
    print("\n" + "-" * 60)
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # ä¾æ¬¡æµ‹è¯•æ¯ä¸ªæ¨¡å‹
    results_summary = []
    for model_config in MODELS_CONFIG:
        model_name, count = run_single_model(
            model_config, 
            questions, 
            num_generations, 
            output_dir
        )
        results_summary.append((model_name, count))
    
    # è®¡ç®—æ€»æ—¶é—´
    total_time = time.time() - start_time
    
    # è¾“å‡ºæ±‡æ€»
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•å®Œæˆæ±‡æ€»")
    print("=" * 60)
    for model_name, count in results_summary:
        print(f"  â€¢ {model_name}: {count} æ¡å¯¹è¯")
    print(f"\nâ±ï¸  æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
    print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³: {output_dir}/")
    print("=" * 60)


if __name__ == "__main__":
    main()